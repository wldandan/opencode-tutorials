# 第3章 技术架构设计基础 实施计划

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** 创建第3章"技术架构设计基础"的完整内容，包含LLM选型决策框架和系统架构模式选择两部分。

**Architecture:** 按照规划的结构，创建2个小节（3.1和3.2），共3个markdown文件。采用案例驱动方式，通过智能客服和企业知识库问答两个代表性案例，展示模型选型决策和架构演进过程。

**Tech Stack:** Markdown文档，遵循现有课程文档的格式规范（标题、元数据、代码块、表格、ASCII图表）

---

## 文档结构

```
chapter03-技术架构设计基础/
├── 3.1-LLM选型决策框架.md
└── 3.2-系统架构模式选择.md
```

---

## Task 1: 创建章节目录

**Files:**
- Create: `002-courses/003-实践级-AI工程实践：从POC到生产/chapter03-技术架构设计基础/`

**Step 1: 创建chapter03目录**

Run: `mkdir -p "002-courses/003-实践级-AI工程实践：从POC到生产/chapter03-技术架构设计基础"`
Expected: 目录创建成功

**Step 2: 验证目录创建**

Run: `ls -la "002-courses/003-实践级-AI工程实践：从POC到生产/" | grep chapter`
Expected: 显示chapter01, chapter02, chapter03

---

## Task 2: 创建3.1节 LLM选型决策框架

**Files:**
- Create: `002-courses/003-实践级-AI工程实践：从POC到生产/chapter03-技术架构设计基础/3.1-LLM选型决策框架.md`

**Step 1: 创建文件并写入头部元数据**

```markdown
# 3.1 LLM选型决策框架

> **所属章节**：第3章 技术架构设计基础
> **阅读时长**：12分钟
> **难度等级**：⭐⭐⭐⭐

---

## 引言

选择合适的LLM（大语言模型）是LLM应用项目的第一个关键技术决策。模型选型直接影响项目效果、成本、可维护性，甚至决定项目成败。

很多团队在模型选型上犯过错误：
- 一开始就用最贵的商用模型，成本失控
- 盲目追求开源模型，效果不达标
- 没有考虑数据隐私和合规要求
- 忽略了模型的迭代速度和生态支持

本文将提供一套系统的LLM选型决策框架，帮助你做出明智的模型选择。

---
```

**Step 2: 添加3.1.1 商用vs开源对比（2分钟）内容**

在文件中添加以下内容：

```markdown
## 3.1.1 商用模型 vs 开源模型

### 快速对比

| 维度 | 商用模型 | 开源模型 |
|-----|---------|---------|
| **效果** | ⭐⭐⭐⭐⭐ 顶尖 | ⭐⭐⭐⭐ 接近商用 |
| **成本** | 按调用付费（高） | 部署成本为主（低） |
| **数据隐私** | 数据上传第三方 | 本地部署（可控） |
| **定制化** | 有限 | 完全可控 |
| **上线速度** | 即用 | 需要部署 |
| **维护成本** | 低 | 中高 |

### 商用模型适用场景

✅ **推荐使用商用模型**：
- 需要最佳效果（如复杂推理任务）
- 数据隐私要求不高
- 调用量不大，成本可接受
- 需要快速上线验证

❌ **不推荐使用商用模型**：
- 有敏感数据不能外传
- 大规模调用，成本敏感
- 需要深度定制模型

### 开源模型适用场景

✅ **推荐使用开源模型**：
- 有数据隐私要求
- 大规模调用，成本敏感
- 有技术团队维护
- 需要微调或定制

❌ **不推荐使用开源模型**：
- 缺乏技术团队
- 需要极致效果
- 快速验证阶段

---
```

**Step 3: 添加3.1.2 国内外模型生态分析（2分钟）内容**

```markdown
## 3.1.2 国内外模型生态

### 国外主流模型

| 模型 | 厂商 | 特点 | 适用场景 |
|-----|------|------|---------|
| **GPT-4** | OpenAI | 效果顶尖，成本高 | 复杂推理、创意写作 |
| **Claude 3** | Anthropic | 长文本优秀，安全性强 | 长文档分析、合规场景 |
| **Llama 3** | Meta | 开源领先，生态好 | 本地部署、成本控制 |

### 国内主流模型

| 模型 | 厂商 | 特点 | 适用场景 |
|-----|------|------|---------|
| **文心一言** | 百度 | 中文优秀，企业服务成熟 | 通用中文场景 |
| **通义千问** | 阿里 | 生态完善，API稳定 | 电商、企业应用 |
| **Qwen** | 阿里 | 开源版本强，技术领先 | 本地部署、定制化 |
| **GLM** | 智谱 | 中文推理强，学术背景 | 复杂中文任务 |

### 生态成熟度对比

```
商用API生态：     GPT-4 ≈ Claude 3 > 文心 > 通义 > GLM
开源生态：        Llama 3 > Qwen > 其他
中文效果：        GPT-4 > Claude 3 > 通义 ≈ Qwen > 文心 > GLM
成本优势：        开源部署 >> 商用API
```

### 选型建议

**快速验证阶段**：优先商用API（GPT-4、Claude 3）
**中文场景**：优先国内模型（通义、Qwen）
**成本敏感**：考虑开源（Llama 3、Qwen）
**数据敏感**：本地部署（Qwen、Llama 3）

---
```

**Step 4: 添加3.1.3 选型决策矩阵（8分钟）内容 - 案例驱动**

```markdown
## 3.1.3 选型决策矩阵

### 决策矩阵框架

| 评估维度 | 权重 | 评分标准（1-5分） |
|---------|------|------------------|
| 效果要求 | 30% | 1=可接受 → 5=顶尖 |
| 成本约束 | 25% | 1=敏感 → 5=不敏感 |
| 数据隐私 | 20% | 1=必须本地 → 5=可上传 |
| 上线速度 | 15% | 1=急 → 5=不急 |
| 技术能力 | 10% | 1=弱 → 5=强 |

**总分计算**：Σ(权重 × 分数)
- 总分 < 2.5：开源模型
- 总分 ≥ 2.5：商用模型

### 案例1：智能客服系统

**场景描述**：
- 某银行计划开发智能客服系统
- 日均咨询量10万次
- 需要处理敏感金融信息
- 有技术团队50人

**评分过程**：

| 评估维度 | 权重 | 评分 | 加权分 | 说明 |
|---------|------|------|--------|------|
| 效果要求 | 30% | 4 | 1.2 | 客服需要高准确度，但不需要极致推理 |
| 成本约束 | 25% | 2 | 0.5 | 10万次/日，商用API成本太高 |
| 数据隐私 | 20% | 1 | 0.2 | 金融数据必须本地处理 |
| 上线速度 | 15% | 3 | 0.45 | 有一定时间压力但非紧急 |
| 技术能力 | 10% | 5 | 0.5 | 50人团队，技术能力强 |
| **总分** | | | **2.85** | |

**决策**：总分 < 2.5 ✅ 选择开源模型

**具体选型**：Qwen-72B（开源中文模型）
- 效果接近GPT-4，中文优化好
- 可本地部署，满足隐私要求
- 一次性部署成本，无调用费用

### 案例2：企业知识库问答

**场景描述**：
- 某创业公司做内部知识库问答
- 日均使用500次
- 数据非敏感（内部文档）
- 技术团队3人

**评分过程**：

| 评估维度 | 权重 | 评分 | 加权分 | 说明 |
|---------|------|------|--------|------|
| 效果要求 | 30% | 3 | 0.9 | 知识库问答不需要极致效果 |
| 成本约束 | 25% | 4 | 1.0 | 500次/日，商用API成本可接受 |
| 数据隐私 | 20% | 5 | 1.0 | 内部文档，无隐私限制 |
| 上线速度 | 15% | 1 | 0.15 | 创业公司，需要快速验证 |
| 技术能力 | 10% | 2 | 0.2 | 3人小团队，维护能力有限 |
| **总分** | | | **3.25** | |

**决策**：总分 ≥ 2.5 ✅ 选择商用模型

**具体选型**：GPT-4o（商用API）
- 效果最佳，用户体验好
- API稳定，无需维护
- 快速上线，专注业务

### 简化决策树（快速参考）

```
是否涉及敏感数据？
├─ 是 → 开源模型（Qwen、Llama 3）
└─ 否 → 是否日均调用 > 1万次？
    ├─ 是 → 开源模型（成本考虑）
    └─ 否 → 是否有技术团队维护？
        ├─ 是 → 开源模型可选
        └─ 否 → 商用模型（GPT-4、Claude 3）
```

---
```

**Step 5: 添加总结和延伸阅读部分**

```markdown
## 总结

### 快速参考

| 你的场景 | 推荐选择 |
|---------|---------|
| 快速验证 | GPT-4o、Claude 3.5 Sonnet |
| 中文场景 | 通义千问、Qwen |
| 成本敏感 | Qwen、Llama 3 本地部署 |
| 数据敏感 | Qwen、Llama 3 本地部署 |
| 复杂推理 | GPT-4、Claude 3 Opus |
| 长文本 | Claude 3、通义千问长文版 |

### 选型三问

1. **效果够用即可**：你的任务是否需要最顶尖的模型？
2. **成本可控**：日均调用量多少？成本是否可接受？
3. **数据安全**：是否有敏感数据不能外传？

---

## 延伸阅读

- [LLM Model Benchmark](https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard)：LLM效果排行榜
- [Qwen模型文档](https://github.com/QwenLM/Qwen)：Qwen开源模型
- [OpenAI API文档](https://platform.openai.com/docs)：商用API参考

---

**上一节**：第2章 需求分析与价值验证
**下一节**：3.2 系统架构模式选择
```

**Step 6: 验证文件内容**

Run: `head -50 "002-courses/003-实践级-AI工程实践：从POC到生产/chapter03-技术架构设计基础/3.1-LLM选型决策框架.md"`
Expected: 显示文件头部内容正常

**Step 7: 提交文件**

```bash
git add "002-courses/003-实践级-AI工程实践：从POC到生产/chapter03-技术架构设计基础/3.1-LLM选型决策框架.md"
git commit -m "feat: add chapter 3.1 LLM selection decision framework"
```

---

## Task 3: 创建3.2节 系统架构模式选择

**Files:**
- Create: `002-courses/003-实践级-AI工程实践：从POC到生产/chapter03-技术架构设计基础/3.2-系统架构模式选择.md`

**Step 1: 创建文件并写入头部元数据**

```markdown
# 3.2 系统架构模式选择

> **所属章节**：第3章 技术架构设计基础
> **阅读时长**：18分钟
> **难度等级**：⭐⭐⭐⭐

---

## 引言

选择好LLM模型后，下一个关键决策是：**如何设计系统架构？**

传统软件架构模式（单体、微服务、Serverless）是基础，而AI应用还有特殊的架构模式（直连、代理、混合）。正确的架构模式能让系统易于维护、扩展和优化；错误的架构模式会让项目陷入技术债务的泥潭。

本文将分两部分：首先快速回顾传统架构模式，然后重点讲解AI应用架构模式的演进。

---
```

**Step 2: 添加3.2.1 传统架构模式对比（6分钟）内容**

```markdown
## 3.2.1 传统架构模式对比

### 三种模式快速对比

| 维度 | 单体架构 | 微服务架构 | Serverless |
|-----|---------|-----------|-----------|
| **复杂度** | 低 | 高 | 中 |
| **开发效率** | 高（初期） | 中 | 高 |
| **维护成本** | 高（后期） | 中 | 低 |
| **扩展性** | 低 | 高 | 自动 |
| **成本** | 低 | 中 | 按量付费 |
| **适用场景** | MVP阶段 | 规模化 | 波动大 |

### LLM应用的建议

```
MVP阶段：     单体架构（快速开发）
              ↓
试点阶段：    微服务架构（模块化）
              ↓
规模阶段：    Serverless + 微服务（按需扩展）
```

**核心原则**：
- 从简单开始，逐步演进
- 避免过度设计（YAGNI原则）
- 1-2-3模型，每个阶段架构可以调整

---
```

**Step 3: 添加3.2.2 AI应用架构演进（12分钟）内容 - 演进式案例**

```markdown
## 3.2.2 AI应用架构演进

### 三种AI应用架构模式

| 模式 | 结构 | 优点 | 缺点 | 适用阶段 |
|-----|------|------|------|---------|
| **直连模式** | 前端 → LLM API | 简单，快速上线 | 无上下文，难扩展 | MVP |
| **代理模式** | 前端 → 后端 → LLM | 可控制，可扩展 | 多一层，延迟增 | 试点 |
| **混合模式** | 多模型 + 路由 | 效果优，成本优 | 复杂，运维难 | 规模 |

### 案例1：智能客服系统架构演进

#### 阶段A：MVP阶段 - 直连模式

```
┌─────────────────────────────────────────────────────────────────┐
│                    直连模式（MVP阶段）                           │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   用户浏览器                                                     │
│      ↓                                                          │
│   简单前端（React）                                              │
│      ↓                                                          │
│   LLM API（GPT-4o）                                             │
│                                                                 │
│   优点：最简单，2人周即可上线                                    │
│   缺点：无法管理prompt，无法记录对话，无法控制成本               │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

**适用场景**：验证用户接受度
**典型代码**：
```javascript
// 前端直接调用LLM API
const response = await fetch('https://api.openai.com/v1/chat', {
  method: 'POST',
  body: JSON.stringify({
    model: 'gpt-4o',
    messages: [{role: 'user', content: userInput}]
  })
});
```

#### 阶段B：试点阶段 - 代理模式

```
┌─────────────────────────────────────────────────────────────────┐
│                    代理模式（试点阶段）                           │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   用户浏览器                                                     │
│      ↓                                                          │
│   前端（React）                                                  │
│      ↓                                                          │
│   ┌─────────────────────────────────┐                          │
│   │   后端服务（Python/FastAPI）    │                          │
│   │                                 │                          │
│   │   - Prompt模板管理               │                          │
│   │   - 对话历史存储                 │                          │
│   │   - 业务逻辑处理                 │                          │
│   │   - 成本监控                     │                          │
│   └─────────────────────────────────┘                          │
│      ↓           ↓           ↓                                 │
│   LLM API    数据库      监控系统                               │
│                                                                 │
│   优点：可控、可扩展、可监控                                     │
│   缺点：多一层，延迟增加50-100ms                                │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

**适用场景**：真实业务验证，需要监控和优化
**典型代码**：
```python
# 后端代理服务
@app.post("/chat")
async def chat(request: ChatRequest):
    # 1. 加载Prompt模板
    prompt = prompt_manager.get("customer_service")

    # 2. 添加对话历史
    messages = db.get_history(request.session_id)
    messages.append({"role": "user", "content": request.message})

    # 3. 调用LLM
    response = await llm_client.chat(messages, prompt=prompt)

    # 4. 存储对话
    db.save_message(request.session_id, request.message, response)

    # 5. 记录成本
    cost_monitor.log(response.usage)

    return {"response": response.content}
```

#### 阶段C：规模阶段 - 混合模式

```
┌─────────────────────────────────────────────────────────────────┐
│                    混合模式（规模阶段）                           │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   用户浏览器                                                     │
│      ↓                                                          │
│   前端（React）                                                  │
│      ↓                                                          │
│   ┌─────────────────────────────────────────────────┐          │
│   │              智能路由层                          │          │
│   │                                                 │          │
│   │   简单问答 ──────→ Qwen-7B（本地，成本低）       │          │
│   │   复杂推理 ──────→ GPT-4o（API，效果好）        │          │
│   │   敏感操作 ──────→ 人工客服                     │          │
│   └─────────────────────────────────────────────────┘          │
│      ↓           ↓           ↓                                 │
│   本地LLM      商用API      人工客服                             │
│                                                                 │
│   优点：成本优化，效果保证，高可用                              │
│   缺点：复杂度高，需要运维                                      │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

**适用场景**：大规模生产，成本敏感
**典型代码**：
```python
# 智能路由
@app.post("/chat")
async def chat(request: ChatRequest):
    # 1. 意图分类
    intent = await intent_classifier.classify(request.message)

    # 2. 路由决策
    if intent == "simple_qa":
        # 简单问答用本地模型
        response = await local_llm.chat(request.message)
    elif intent == "complex_reasoning":
        # 复杂推理用GPT-4
        response = await gpt4.chat(request.message)
    elif intent == "sensitive":
        # 敏感操作转人工
        response = await human_handover(request)

    return {"response": response}
```

### 案例2：企业知识库问答架构演进

#### MVP阶段：简单RAG（直连模式）

```
用户问题 → 向量检索 → LLM生成 → 返回答案
```

#### 试点阶段：完整RAG（代理模式）

```
用户问题
    ↓
后端服务
    ↓
┌─────────────────────────────────┐
│   RAG Pipeline                  │
│                                 │
│   1. 意图识别                   │
│   2. 检索增强（向量+关键词）    │
│   3. Prompt组装                 │
│   4. LLM生成                    │
│   5. 答案验证                   │
└─────────────────────────────────┘
    ↓
返回答案 + 引用来源
```

#### 规模阶段：混合RAG（混合模式）

```
用户问题
    ↓
智能路由
    ↓
┌──────────┬──────────┬──────────┐
│ 简单问答  │ 文档问答  │ 数据分析  │
│ ↓        │ ↓        │ ↓        │
│ 缓存     │ RAG      │ Agent+SQL │
└──────────┴──────────┴──────────┘
```

### 架构演进决策矩阵

| 阶段 | 并发量 | 团队规模 | 推荐架构 | 演进时机 |
|-----|-------|---------|---------|---------|
| MVP | < 100 | 1-3人 | 直连 | 用户验证通过 |
| 试点 | 100-1K | 3-10人 | 代理 | 效果达标，准备推广 |
| 规模 | > 1K | 10+人 | 混合 | 成本/性能压力 |

### 架构演进常见误区

❌ **错误1**：一开始就上微服务+混合架构
- 后果：过度设计，开发周期长
- 正确：从简单开始，逐步演进

❌ **错误2**：MVP阶段上复杂监控
- 后果：重心偏移，验证延迟
- 正确：MVP阶段只需基本日志

❌ **错误3**：架构固化，不愿调整
- 后果：技术债务累积
- 正确：每个阶段都是架构调整点

---
```

**Step 4: 添加总结部分**

```markdown
## 总结

### 架构选择速查表

| 你的场景 | 推荐架构 |
|---------|---------|
| MVP验证 | 直连模式 + 单体架构 |
| 试点部署 | 代理模式 + 微服务架构 |
| 规模生产 | 混合模式 + Serverless |
| 成本敏感 | 本地LLM优先 |
| 效果优先 | 商用API + 智能路由 |

### 架构演进三原则

1. **从简单开始**：避免过度设计
2. **逐步演进**：每个阶段评估架构是否需要调整
3. **数据驱动**：基于实际数据（并发、成本、效果）做架构决策

### 与1-2-3模型的对应

| 阶段 | 架构特点 |
|-----|---------|
| 1评估（机会发现） | 无需架构设计 |
| 2验证（POC+MVP） | 直连模式，快速验证 |
| 3增长（试点+规模+持续） | 代理→混合，逐步复杂化 |

---

## 延伸阅读

- 《Designing Data-Intensive Applications》：架构设计经典
- LLMOps最佳实践：LLM应用的运维模式
- FastAPI官方文档：构建高性能后端服务

---

**上一节**：3.1 LLM选型决策框架
**下一节**：第4章 工程实践核心技能
```

**Step 5: 验证文件内容**

Run: `head -50 "002-courses/003-实践级-AI工程实践：从POC到生产/chapter03-技术架构设计基础/3.2-系统架构模式选择.md"`
Expected: 显示文件头部内容正常

**Step 6: 提交文件**

```bash
git add "002-courses/003-实践级-AI工程实践：从POC到生产/chapter03-技术架构设计基础/3.2-系统架构模式选择.md"
git commit -m "feat: add chapter 3.2 system architecture pattern selection"
```

---

## Task 4: 更新课程主README

**Files:**
- Modify: `002-courses/003-实践级-AI工程实践：从POC到生产/README.md`

**Step 1: 读取当前README内容**

Run: `cat "002-courses/003-实践级-AI工程实践：从POC到生产/README.md"`
Expected: 显示当前README内容

**Step 2: 在目录中添加第3章条目**

在目录部分添加：

```markdown
## 第3章 技术架构设计基础（30分钟，⭐⭐⭐⭐）

### 3.1 LLM选型决策框架（12分钟）
- 商用模型 vs 开源模型
- 国内外模型生态分析
- 选型决策矩阵（案例：智能客服、知识库问答）

### 3.2 系统架构模式选择（18分钟）
- 传统架构模式对比（单体/微服务/Serverless）
- AI应用架构演进（直连→代理→混合）
- 案例演进：智能客服、知识库问答

---
```

**Step 3: 提交更新**

```bash
git add "002-courses/003-实践级-AI工程实践：从POC到生产/README.md"
git commit -m "docs: update README with chapter 3 outline"
```

---

## 验证清单

完成所有任务后，验证以下内容：

- [ ] chapter03目录已创建
- [ ] 3.1-LLM选型决策框架.md 内容完整（约12分钟阅读量）
- [ ] 3.2-系统架构模式选择.md 内容完整（约18分钟阅读量）
- [ ] 两个文件格式与现有章节一致
- [ ] 术语使用与第1-2章保持一致（"企业AI工程框架"、"1-2-3模型"等）
- [ ] 案例一致性：智能客服和知识库问答在3.1和3.2中保持一致
- [ ] README.md已更新
- [ ] 所有更改已提交到git

---

## 关键注意事项

1. **术语一致性**：
   - 使用"企业AI工程框架"或"1-2-3模型"
   - "AI工程实践"指代"LLM应用工程实践"
   - "LLM应用项目"而非"AI项目"

2. **案例一致性**：
   - 智能客服：3.1模型选型 + 3.2架构演进
   - 知识库问答：3.1模型选型 + 3.2架构演进

3. **格式规范**：
   - 使用与第1-2章相同的格式
   - 标题使用`##`（二级标题）
   - 代码块使用```标记语言
   - 表格对齐

4. **内容深度**：
   - ⭐⭐⭐⭐技术深度，适合有一定基础的读者
   - 案例驱动，避免纯理论
   - 提供可操作的决策框架
