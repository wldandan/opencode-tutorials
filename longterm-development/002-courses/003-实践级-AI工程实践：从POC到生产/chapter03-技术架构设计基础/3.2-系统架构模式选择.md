# 3.2 系统架构模式选择

> **所属章节**：第3章 技术架构设计基础
> **阅读时长**：18分钟
> **难度等级**：⭐⭐⭐⭐

---

## 引言

选择好LLM模型后，下一个关键决策是：**如何设计系统架构？**

传统软件架构模式（单体、微服务、Serverless）是基础，而AI应用还有特殊的架构模式（直连、代理、混合）。正确的架构模式能让系统易于维护、扩展和优化；错误的架构模式会让项目陷入技术债务的泥潭。

本文将分两部分：首先快速回顾传统架构模式，然后重点讲解AI应用架构模式的演进。

---

## 3.2.1 传统架构模式对比

### 三种模式快速对比

| 维度 | 单体架构 | 微服务架构 | Serverless |
|-----|---------|-----------|-----------|
| **复杂度** | 低 | 高 | 中 |
| **开发效率** | 高（初期） | 中 | 高 |
| **维护成本** | 高（后期） | 中 | 低 |
| **扩展性** | 低 | 高 | 自动 |
| **成本** | 低 | 中 | 按量付费 |
| **适用场景** | MVP阶段 | 规模化 | 波动大 |

### LLM应用的建议

```
MVP阶段：     单体架构（快速开发）
              ↓
试点阶段：    微服务架构（模块化）
              ↓
规模阶段：    Serverless + 微服务（按需扩展）
```

**核心原则**：
- 从简单开始，逐步演进
- 避免过度设计（YAGNI原则）
- 1-2-3模型，每个阶段架构可以调整

---

## 3.2.2 AI应用架构演进

### 三种AI应用架构模式

| 模式 | 结构 | 优点 | 缺点 | 适用阶段 |
|-----|------|------|------|---------|
| **直连模式** | 前端 → LLM API | 简单，快速上线 | 无上下文，难扩展 | MVP |
| **代理模式** | 前端 → 后端 → LLM | 可控制，可扩展 | 多一层，延迟增 | 试点 |
| **混合模式** | 多模型 + 路由 | 效果优，成本优 | 复杂，运维难 | 规模 |

### 案例1：智能客服系统架构演进

#### 阶段A：MVP阶段 - 直连模式

```
┌─────────────────────────────────────────────────────────────────┐
│                    直连模式（MVP阶段）                           │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   用户浏览器                                                     │
│      ↓                                                          │
│   简单前端（React）                                              │
│      ↓                                                          │
│   LLM API（GPT-4o）                                             │
│                                                                 │
│   优点：最简单，2人周即可上线                                    │
│   缺点：无法管理prompt，无法记录对话，无法控制成本               │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

**适用场景**：验证用户接受度
**典型代码**：
```javascript
// 前端直接调用LLM API
const response = await fetch('https://api.openai.com/v1/chat', {
  method: 'POST',
  body: JSON.stringify({
    model: 'gpt-4o',
    messages: [{role: 'user', content: userInput}]
  })
});
```

#### 阶段B：试点阶段 - 代理模式

```
┌─────────────────────────────────────────────────────────────────┐
│                    代理模式（试点阶段）                           │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   用户浏览器                                                     │
│      ↓                                                          │
│   前端（React）                                                  │
│      ↓                                                          │
│   ┌─────────────────────────────────┐                          │
│   │   后端服务（Python/FastAPI）    │                          │
│   │                                 │                          │
│   │   - Prompt模板管理               │                          │
│   │   - 对话历史存储                 │                          │
│   │   - 业务逻辑处理                 │                          │
│   │   - 成本监控                     │                          │
│   └─────────────────────────────────┘                          │
│      ↓           ↓           ↓                                 │
│   LLM API    数据库      监控系统                               │
│                                                                 │
│   优点：可控、可扩展、可监控                                     │
│   缺点：多一层，延迟增加50-100ms                                │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

**适用场景**：真实业务验证，需要监控和优化
**典型代码**：
```python
# 后端代理服务
@app.post("/chat")
async def chat(request: ChatRequest):
    # 1. 加载Prompt模板
    prompt = prompt_manager.get("customer_service")

    # 2. 添加对话历史
    messages = db.get_history(request.session_id)
    messages.append({"role": "user", "content": request.message})

    # 3. 调用LLM
    response = await llm_client.chat(messages, prompt=prompt)

    # 4. 存储对话
    db.save_message(request.session_id, request.message, response)

    # 5. 记录成本
    cost_monitor.log(response.usage)

    return {"response": response.content}
```

#### 阶段C：规模阶段 - 混合模式

```
┌─────────────────────────────────────────────────────────────────┐
│                    混合模式（规模阶段）                           │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│   用户浏览器                                                     │
│      ↓                                                          │
│   前端（React）                                                  │
│      ↓                                                          │
│   ┌─────────────────────────────────────────────────┐          │
│   │              智能路由层                          │          │
│   │                                                 │          │
│   │   简单问答 ──────→ Qwen-7B（本地，成本低）       │          │
│   │   复杂推理 ──────→ GPT-4o（API，效果好）        │          │
│   │   敏感操作 ──────→ 人工客服                     │          │
│   └─────────────────────────────────────────────────┘          │
│      ↓           ↓           ↓                                 │
│   本地LLM      商用API      人工客服                             │
│                                                                 │
│   优点：成本优化，效果保证，高可用                              │
│   缺点：复杂度高，需要运维                                      │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

**适用场景**：大规模生产，成本敏感
**典型代码**：
```python
# 智能路由
@app.post("/chat")
async def chat(request: ChatRequest):
    # 1. 意图分类
    intent = await intent_classifier.classify(request.message)

    # 2. 路由决策
    if intent == "simple_qa":
        # 简单问答用本地模型
        response = await local_llm.chat(request.message)
    elif intent == "complex_reasoning":
        # 复杂推理用GPT-4
        response = await gpt4.chat(request.message)
    elif intent == "sensitive":
        # 敏感操作转人工
        response = await human_handover(request)

    return {"response": response}
```

### 案例2：企业知识库问答架构演进

#### MVP阶段：简单RAG（直连模式）

```
用户问题 → 向量检索 → LLM生成 → 返回答案
```

#### 试点阶段：完整RAG（代理模式）

```
用户问题
    ↓
后端服务
    ↓
┌─────────────────────────────────┐
│   RAG Pipeline                  │
│                                 │
│   1. 意图识别                   │
│   2. 检索增强（向量+关键词）    │
│   3. Prompt组装                 │
│   4. LLM生成                    │
│   5. 答案验证                   │
└─────────────────────────────────┘
    ↓
返回答案 + 引用来源
```

#### 规模阶段：混合RAG（混合模式）

```
用户问题
    ↓
智能路由
    ↓
┌──────────┬──────────┬──────────┐
│ 简单问答  │ 文档问答  │ 数据分析  │
│ ↓        │ ↓        │ ↓        │
│ 缓存     │ RAG      │ Agent+SQL │
└──────────┴──────────┴──────────┘
```

### 架构演进决策矩阵

| 阶段 | 并发量 | 团队规模 | 推荐架构 | 演进时机 |
|-----|-------|---------|---------|---------|
| MVP | < 100 | 1-3人 | 直连 | 用户验证通过 |
| 试点 | 100-1K | 3-10人 | 代理 | 效果达标，准备推广 |
| 规模 | > 1K | 10+人 | 混合 | 成本/性能压力 |

### 架构演进常见误区

❌ **错误1**：一开始就上微服务+混合架构
- 后果：过度设计，开发周期长
- 正确：从简单开始，逐步演进

❌ **错误2**：MVP阶段上复杂监控
- 后果：重心偏移，验证延迟
- 正确：MVP阶段只需基本日志

❌ **错误3**：架构固化，不愿调整
- 后果：技术债务累积
- 正确：每个阶段都是架构调整点

---

## 总结

### 架构选择速查表

| 你的场景 | 推荐架构 |
|---------|---------|
| MVP验证 | 直连模式 + 单体架构 |
| 试点部署 | 代理模式 + 微服务架构 |
| 规模生产 | 混合模式 + Serverless |
| 成本敏感 | 本地LLM优先 |
| 效果优先 | 商用API + 智能路由 |

### 架构演进三原则

1. **从简单开始**：避免过度设计
2. **逐步演进**：每个阶段评估架构是否需要调整
3. **数据驱动**：基于实际数据（并发、成本、效果）做架构决策

### 与1-2-3模型的对应

| 阶段 | 架构特点 |
|-----|---------|
| 1评估（机会发现） | 无需架构设计 |
| 2验证（POC+MVP） | 直连模式，快速验证 |
| 3增长（试点+规模+持续） | 代理→混合，逐步复杂化 |

---

## 延伸阅读

- 《Designing Data-Intensive Applications》：架构设计经典
- LLMOps最佳实践：LLM应用的运维模式
- FastAPI官方文档：构建高性能后端服务

---

**上一节**：3.1 LLM选型决策框架
**下一节**：第4章 工程实践核心技能
